{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 개요\n",
    "# 신경망 이슈들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 샘플링 방법\n",
    "- 배치 학습 (Batch Learning) : 전체 데이터에 대해 손실값 적용 후 가중치 변경 (기존 방법)\n",
    "- 확률적 경사하강법 (SGD, Stochastic Gradient Descent) : 샘플 하나하나에 대해 가중치 변경\n",
    "- 미니배치(Minibatch) : 복수의 샘플로 묶어 가중치 변경 (가장 일반적인 방법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://datascience-enthusiast.com/DL/Optimization_methods.html\n",
    "\n",
    "<img src=\"https://datascience-enthusiast.com/figures/kiank_sgd.png\"><br>\n",
    "<img src=\"https://datascience-enthusiast.com/figures/kiank_minibatch.png\"><br>\n",
    "- momentum\n",
    "<img src=\"https://datascience-enthusiast.com/figures/opt_momentum.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 종류\n",
    "- https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD (Stochastic Gradient Descent)\n",
    "- 'Stochastic' 은 랜덤하게 샘플을 뽑는다는 의미임\n",
    "- 기본적인 경사하강법을 샘플에 적용함\n",
    "- 추가적으로 모멘텀 적용 가능\n",
    "> $ W(t) = W(t-1) - \\eta \\cdot \\nabla_W Loss + \\alpha \\cdot \\Delta W(t-1) $\n",
    "\n",
    "<img src='https://tensorflowkorea.files.wordpress.com/2017/03/ec8aa4ed81aceba6b0ec83b7-2017-03-21-ec98a4ed9b84-3-22-52.png?w=625' />\n",
    "\n",
    "#### Adagrad\n",
    "- 각 가중치(w) 마다 학습률을 다르게 설정\n",
    "- 변화가 많았던 가중치는 적게, 변화가 적었던 가중치는 많게 학습률을 적용한다.\n",
    "> $ W(t) = W(t-1) - { \\eta \\over \\sqrt{\\sum \\nabla Loss} } \\cdot \\nabla_W Loss $\n",
    "\n",
    "#### RMSprop\n",
    "- Adagrad 의 단점을 개선한 알고리즘\n",
    "- 학습이 진행될수록 학습률이 너무 작아지는 경향을 보정<br>\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/2964cc8dc82a134dd4f20e42094f56410b0d2d9c' />\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/fc46ae8619e71130c6c8212eec31560cb4891c0a' />\n",
    "\n",
    "#### Adam\n",
    "- 모멘텀과 RMSprop 의 장점을 결합<br>\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/e388b9155519b8769930b3764f4dadc20eb593b8' />\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/034d5652b502094ab7f58f95a383e0ec41de5b77' />\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/1625bff4ce904cc83c3cadad4bc1a2ff61422b02' />\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/7c5ea1207fc3574a51d439f84370a989deffa871' />\n",
    "> <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/abcd4c729bac933249992e086fa1ba7807e1cd09' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과대적합\n",
    "- 네트워크 크기 축소\n",
    "> . 층의 갯수와 각 층의 뉴런 갯수를 줄인다.<br>\n",
    "> . 층의 갯수와 뉴런 갯수를 정하는 공식은 없기 때문에, 경험적으로 최선의 값을 찾아내어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치 규제 추가\n",
    "> . 최종 손실값에 각 가중치의 제곱(L2규제)나 절대값(L1규제)를 추가한다.<br>\n",
    "> . 규제가 클수록 과대적합이 적어진다.<br>\n",
    "> . 규제가 클수록 w 와 b 값이 0에 가까워진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropout 적용\n",
    "> . 각 층의 출력값 중 일부를 0 으로 만든다.<br>\n",
    "> . 전체 신경망의 예외 대처능력을 키워준다.<br>\n",
    "> . 예측시에는 모든 뉴런을 사용하지만 드롭아웃 비율만큼 전달값을 줄여준다.<br>\n",
    "> . 훈련 시간은 늘어나지만 훨씬 유연한 시스템을 구축할 수 있다.<br>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*iWQzxhVlvadk6VAJjsgXgg.png\" />\n",
    "(출처: https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규화\n",
    "- 신경망도 기본적으로 정규화를 적용하여야 한다\n",
    "- 정규화를 적용하지 않으면 각 속성별로 학습률의 적용효과가 차이가 난다\n",
    "- 입력값이 스케일이 다르면 학습률을 적절히 조절해 주어야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습률 결정\n",
    "- 학습률이 크면 발산할 수 있고, 적으면 학습이 너무 느려질수 있다\n",
    "- 학습 초기에는 학습률을 크게하고, 차차 학습률을 줄여가는 방법이 있다\n",
    "- 출력층에 가까울수록 학습률을 낮추고, 아래층으로 갈수록 학습률을 높여나가는 방법도 적용할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 초기화\n",
    "- 일반적으로 정규분포로 초기화한다\n",
    "- 초기값에 따라 지역최소값에 갇힐 수도 있고 학습이 빨라질수도 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 소실 문제 (Vanishing gradient)\n",
    "- 신경망이 깊을수록 역전파 시 가중치 변경값이 급속도로 줄어드는 경향이 발생한다\n",
    "- 사전훈련(오토인코더)를 통해 가중치 초기값을 설정하여 해결하는 방법이 있다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
